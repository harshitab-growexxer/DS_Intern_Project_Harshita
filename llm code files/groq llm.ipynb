{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10065,"status":"ok","timestamp":1720763090358,"user":{"displayName":"Harshita Balani","userId":"12716172549264531978"},"user_tz":-330},"id":"kd3xCiWZYIDx","outputId":"f7e95f7d-c462-4214-8e3a-3c31c74c8472"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting groq\n","  Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from groq)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.8.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.7)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.6.2)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.20.0)\n","Installing collected packages: h11, httpcore, httpx, groq\n","Successfully installed groq-0.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0\n"]}],"source":["pip install groq"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install configparser"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":719,"status":"ok","timestamp":1720764231748,"user":{"displayName":"Harshita Balani","userId":"12716172549264531978"},"user_tz":-330},"id":"P5OngySTX3mm"},"outputs":[],"source":["import os\n","from groq import Groq\n","import json\n","import duckdb\n","import sqlparse\n","import numpy as np\n","import pandas as pd\n","import psycopg2\n","import configparser"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1720764232616,"user":{"displayName":"Harshita Balani","userId":"12716172549264531978"},"user_tz":-330},"id":"txr4VYsHYT38"},"outputs":[],"source":["config = configparser.ConfigParser()\n","config.read('config.ini')\n","\n","PGEND_POINT = config['database']['PGEND_POINT']\n","PGUSER_NAME = config['database']['PGUSER_NAME']\n","PGPASSWORD = config['database']['PGPASSWORD']\n","PGDATABASE_NAME = config['database']['PGDATABASE_NAME']"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1720764232616,"user":{"displayName":"Harshita Balani","userId":"12716172549264531978"},"user_tz":-330},"id":"gNNMwX-MYGio"},"outputs":[],"source":["def connect():\n","    # Set up a connection to the PostgreSQL server.\n","    conn_string = \"host=\" + PGEND_POINT + \" port=5432 dbname=\" + PGDATABASE_NAME + \\\n","                  \" user=\" + PGUSER_NAME + \" password=\" + PGPASSWORD\n","\n","    conn = psycopg2.connect(conn_string)\n","    print(\"Connected!\")\n","\n","    # Create a cursor object\n","    cursor = conn.cursor()\n","\n","    return conn, cursor\n","\n","def execute_postgresql_query(query):\n","    conn, cursor = connect()\n","    try:\n","        cursor.execute(query)\n","        columns = [desc[0] for desc in cursor.description]\n","        query_result = pd.DataFrame(cursor.fetchall(), columns=columns)\n","    finally:\n","        conn.close()\n","    return query_result"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1720764232617,"user":{"displayName":"Harshita Balani","userId":"12716172549264531978"},"user_tz":-330},"id":"3m3sifEbYB06"},"outputs":[],"source":["def chat_with_groq(client, prompt, model, response_format):\n","    completion = client.chat.completions.create(\n","        model=model,\n","        messages=[\n","            {\n","                \"role\": \"user\",\n","                \"content\": f\"{prompt}\\n\\nPlease provide the response in JSON format.\"\n","            }\n","        ],\n","        response_format=response_format\n","    )\n","    return completion.choices[0].message.content"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1720764233370,"user":{"displayName":"Harshita Balani","userId":"12716172549264531978"},"user_tz":-330},"id":"MiiOcthDYQny"},"outputs":[],"source":["def get_summarization(client, user_question, df, model):\n","    \"\"\"\n","    This function generates a summarization prompt based on the user's question and the resulting data.\n","    It then sends this summarization prompt to the Groq API and retrieves the AI's response.\n","    \"\"\"\n","    prompt = '''\n","    A user asked the following question pertaining to local database tables:\n","\n","    {user_question}\n","\n","    To answer the question, a dataframe was returned:\n","\n","    Dataframe:\n","    {df}\n","\n","    In a few sentences, summarize the data in the table as it pertains to the original user question. Avoid qualifiers like \"based on the data\" and do not comment on the structure or metadata of the table itself.\n","    '''.format(user_question=user_question, df=df)\n","\n","    # Response format is set to 'None'\n","    return chat_with_groq(client, prompt, model, None)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1720764233370,"user":{"displayName":"Harshita Balani","userId":"12716172549264531978"},"user_tz":-330},"id":"9Ypjh_DrYV1G"},"outputs":[],"source":["# Use the Llama3 70b model\n","model = \"llama3-70b-8192\"\n","\n","# Get the Groq API key and create a Groq client\n","groq_api_key = os.environ['api']['groq_api_key']\n","client = Groq(\n","  api_key=groq_api_key\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1720764233371,"user":{"displayName":"Harshita Balani","userId":"12716172549264531978"},"user_tz":-330},"id":"rXe5ehIyYXnA","outputId":"5161808b-31fe-4956-e7dd-58686ccc4ad1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Welcome to the Query Generator!\n","You can ask questions about the data\n"]}],"source":["print(\"Welcome to the Query Generator!\")\n","print(\"You can ask questions about the data\")\n","\n","base_prompt = \"Given a user's question about this data, write a valid SQL query that accurately extracts or calculates the requested information from the 'turbotable' and adheres to SQL best practices for AWS RDS, optimizing for readability and performance where applicable.\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDFGicj8YZqm","outputId":"1d824be4-1b63-45f4-c8ab-efd2de8d7850"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ask a question: maximum value of altitude\n","\n","Given a user's question about this data, write a valid SQL query that accurately extracts or calculates the requested information from the 'turbotable' and adheres to SQL best practices for AWS RDS, optimizing for readability and performance where applicable. Here is the user's question:\n","maximum value of altitude\n","\n"]}],"source":["base_prompt = \"\"\"\n","Given a user's question about this data, write a valid SQL query that accurately extracts or calculates the requested information from the 'turbotable' and adheres to SQL best practices for AWS RDS, optimizing for readability and performance where applicable. Here is the user's question:\n","{user_question}\n","\"\"\"\n","\n","while True:\n","    # Get the user's question\n","    user_question = input(\"Ask a question: \")\n","\n","    if user_question:\n","        # Generate the full prompt for the AI\n","        full_prompt = base_prompt.format(user_question=user_question)\n","\n","        print(full_prompt)\n","\n","        # Get the AI's response. Call with '{\"type\": \"json_object\"}' to use JSON mode\n","        llm_response = chat_with_groq(client, full_prompt, model, {\"type\": \"json_object\"})\n","\n","        result_json = json.loads(llm_response)\n","        if 'sql' in result_json:\n","            sql_query = result_json['sql']\n","            results_df = execute_postgresql_query(sql_query)\n","\n","            formatted_sql_query = sqlparse.format(sql_query, reindent=True, keyword_case='upper')\n","\n","            print(\"```sql\\n\" + formatted_sql_query + \"\\n```\")\n","            print(results_df.to_markdown(index=False))\n","\n","            summarization = get_summarization(client, user_question, results_df, model)\n","            print(summarization)\n","        elif 'error' in result_json:\n","            print(\"ERROR:\", 'Could not generate valid SQL for this question')\n","            print(result_json['error'])"]},{"cell_type":"markdown","metadata":{"id":"h8aA3myZdyuw"},"source":["# First run"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"elapsed":11166,"status":"error","timestamp":1720764215567,"user":{"displayName":"Harshita Balani","userId":"12716172549264531978"},"user_tz":-330},"id":"X_OMAwgMc5Nc","outputId":"559473e8-26bf-45c5-e260-78c04f0b63c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Welcome to the DuckDB Query Generator!\n","You can ask questions about the data in the 'employees.csv' and 'purchases.csv' files.\n","Ask a question: maximum value of altitude\n"]},{"ename":"BadRequestError","evalue":"Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error'}}","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-c972040cfd45>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Get the AI's response. Call with '{\"type\": \"json_object\"}' to use JSON mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mllm_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_with_groq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"json_object\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mresult_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-c972040cfd45>\u001b[0m in \u001b[0;36mchat_with_groq\u001b[0;34m(client, prompt, model, response_format)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchat_with_groq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         messages=[\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    287\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \"\"\"\n\u001b[0;32m--> 289\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         )\n\u001b[0;32m-> 1225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 920\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    921\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         return self._process_response(\n","\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error'}}"]}],"source":["import os\n","from groq import Groq\n","import json\n","import sqlparse\n","import psycopg2\n","\n","def chat_with_groq(client, prompt, model, response_format):\n","    completion = client.chat.completions.create(\n","        model=model,\n","        messages=[\n","            {\n","                \"role\": \"user\",\n","                \"content\": prompt\n","            }\n","        ],\n","        response_format=response_format\n","    )\n","\n","    return completion.choices[0].message.content\n","\n","def print_sql_query(query):\n","    \"\"\"\n","    This function prints the SQL query without executing it.\n","    \"\"\"\n","    formatted_sql_query = sqlparse.format(query, reindent=True, keyword_case='upper')\n","    print(\"\\nSQL Query:\\n\" + formatted_sql_query + \"\\n\")\n","\n","def get_summarization(client, user_question, df, model):\n","    \"\"\"\n","    This function generates a summarization prompt based on the user's question and the resulting data.\n","    It then sends this summarization prompt to the Groq API and retrieves the AI's response.\n","\n","    Parameters:\n","    client (Groqcloud): The Groq API client.\n","    user_question (str): The user's question.\n","    df (DataFrame): The DataFrame resulting from the SQL query.\n","    model (str): The AI model to use for the response.\n","\n","    Returns:\n","    str: The content of the AI's response to the summarization prompt.\n","    \"\"\"\n","    prompt = '''\n","      A user asked the following question pertaining to local database tables:\n","\n","      {user_question}\n","\n","      To answer the question, a dataframe was returned:\n","\n","      Dataframe:\n","      {df}\n","\n","    In a few sentences, summarize the data in the table as it pertains to the original user question. Avoid qualifiers like \"based on the data\" and do not comment on the structure or metadata of the table itself.\n","    '''.format(user_question=user_question, df=df)\n","\n","    # Response format is set to 'None'\n","    return chat_with_groq(client, prompt, model, None)\n","\n","# Use the Llama3 70b model\n","model = \"llama3-70b-8192\"\n","\n","# Get the Groq API key and create a Groq client\n","groq_api_key = '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'\n","client = Groq(api_key=groq_api_key)\n","\n","print(\"Welcome to the DuckDB Query Generator!\")\n","print(\"You can ask questions about the data in the 'employees.csv' and 'purchases.csv' files.\")\n","\n","# Load the base prompt\n","base_prompt = \"\"\"\n","Given a user's question about this data, write a valid SQL query that accurately extracts or calculates the requested information from the 'turbotable' and adheres to SQL best practices for AWS RDS, optimizing for readability and performance where applicable. Here is the user's question:\n","{user_question}\n","\"\"\"\n","\n","while True:\n","    # Get the user's question\n","    user_question = input(\"Ask a question: \")\n","\n","    if user_question:\n","        # Generate the full prompt for the AI\n","        full_prompt = base_prompt.format(user_question=user_question)\n","\n","        # Get the AI's response. Call with '{\"type\": \"json_object\"}' to use JSON mode\n","        llm_response = chat_with_groq(client, full_prompt, model, {\"type\": \"json_object\"})\n","\n","        result_json = json.loads(llm_response)\n","        if 'sql' in result_json:\n","            sql_query = result_json['sql']\n","            print_sql_query(sql_query)\n","\n","            # PGEND_POINT = 'databaseinstance.crooqm2eco97.eu-north-1.rds.amazonaws.com' # End_point\n","# PGDATABASE_NAME = 'TestDB' # Database Name example: youtube_test_db\n","# PGUSER_NAME = 'postgres' # UserName\n","# PGPASSWORD = 'Root.123'\n","\n","            # Connect to the PostgreSQL database (if needed for other purposes)\n","            conn = psycopg2.connect(\n","                host=\"databaseinstance.crooqm2eco97.eu-north-1.rds.amazonaws.com\",\n","                port=\"5432\",\n","                dbname=\"TestDB\",\n","                user=\"postgres\",\n","                password=\"Root.123\"\n","            )\n","\n","            # Optional: you can fetch data from the database if needed\n","            # cursor = conn.cursor()\n","            # cursor.execute(sql_query)\n","            # results_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n","            # cursor.close()\n","            # conn.close()\n","            #\n","            # print(results_df.to_markdown(index=False))\n","            #\n","            # summarization = get_summarization(client, user_question, results_df, model)\n","            # print(summarization)\n","        elif 'error' in result_json:\n","            print(\"ERROR:\", 'Could not generate valid SQL for this question')\n","            print(result_json['error'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YiqJSPu1jcvu"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPGb6yFHP0z6n+FfOi8FrBE","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
